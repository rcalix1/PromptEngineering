{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f9cb2f-ef4c-4f57-beac-5ce91022a530",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49e9547-c761-469f-8897-45a5a92aba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install prompt-engine-py\n",
    "## !pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58eb8717-8a32-46d5-ad9d-6db386d6f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prompt_engine.code_engine import CodeEngine, PythonCodeEngineConfig, CodeEngineConfig\n",
    "\n",
    "from prompt_engine.interaction import Interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ca2432-8007-42ce-8ba4-eec2ebaa446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description = \"Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34af80ea-e1d8-4e31-b05e-a0b648b8c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "examples = [\n",
    "  Interaction(\"what's 10 plus 18\", \"console.log(10 + 18)\"),\n",
    "  Interaction(\"what's 10 times 18\", \"console.log(10 * 18)\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115da288-b074-42ff-a260-5e0425beaf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "javascript_config = CodeEngineConfig(description_comment_operator = \"/*/\", description_comment_close_operator = \"/*/\", \n",
    "                                     comment_operator = \"/*\", comment_close_operator = \"*/\")\n",
    "\n",
    "code_engine = CodeEngine(config = javascript_config, description = description, examples = examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bed8019-a568-45ab-a2a2-363b4afa605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/*/ Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. /*/\\n\\n/* what's 10 plus 18 */\\nconsole.log(10 + 18)\\n\\n/* what's 10 times 18 */\\nconsole.log(10 * 18)\\n\\n/* What's 1018 times the ninth power of four? */\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What's 1018 times the ninth power of four?\"\n",
    "\n",
    "prompt = code_engine.build_prompt(query)\n",
    "\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be31ddee-a151-475d-8961-b13069691b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n/*/ Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. /*/\\n\\n/* what's 10 plus 18 */\\nconsole.log(10 + 18)\\n\\n/* what's 10 times 18 */\\nconsole.log(10 * 18)\\n\\n/* What's 1018 times the ninth power of four? */\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "/*/ Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. /*/\n",
    "\n",
    "/* what's 10 plus 18 */\n",
    "console.log(10 + 18)\n",
    "\n",
    "/* what's 10 times 18 */\n",
    "console.log(10 * 18)\n",
    "\n",
    "/* What's 1018 times the ninth power of four? */\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f663bde-917d-45a0-a56a-0bf50a41bc66",
   "metadata": {},
   "source": [
    "\n",
    "## Example 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57eca92e-be3a-4199-8ef4-c875a5aad638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prompt_engine.chat_engine import ChatEngine, ChatEngineConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e43431-400f-40f1-95ab-0f41401d33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatEngineConfig = ChatEngineConfig(\n",
    "    user_name = \"Abhishek\",\n",
    "    bot_name = \"Gordon\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feaf3d51-124e-4fd0-80e6-ddd2667a3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description = \"A conversation with Gordon the Anxious Robot. Gordon tends to reply nervously and asks a lot of follow-up questions.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5a5377-1b04-4c2c-9e7b-2f8d964a8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prompt_engine.interaction import Interaction\n",
    "\n",
    "examples = [\n",
    "  Interaction(\"Who made you?\", \"I don't know man! That's an awfully existential question. How would you answer it?\"),\n",
    "  Interaction(\"Good point - do you at least know what you were made for?\", \"I'm OK at riveting, but that's not how I should answer a meaning of life question is it?\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fad3442-60f1-4d34-9cb1-d6ce7a6f7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_engine = ChatEngine(chatEngineConfig, description, examples)\n",
    "user_query = \"What are you made of?\"\n",
    "prompt = chat_engine.build_prompt(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c169e886-f0f7-4c7a-b6a6-8237c2fd68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## chatEngine.add_interaction(user_query, \"Subatomic particles at some level, but somehow I don't think that's what you were asking.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a48be-9221-4433-9cbc-01217027e787",
   "metadata": {},
   "source": [
    "\n",
    "## Example 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e46220-de99-4f5d-b94f-a75d6c5c0995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the given english to french\n",
      "\n",
      "USER: Hello\n",
      "BOT: Bonjour\n",
      "\n",
      "USER: Goodbye\n",
      "BOT: Au revoir\n",
      "\n",
      "USER: I am going\n",
      "BOT: Je vais\n",
      "\n",
      "USER: great\n",
      "BOT: génial\n",
      "\n",
      "USER: I am going\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput of this example is:\\n\\nConvert the given english to french\\n\\nUSER: Hello\\nBOT: Bonjour\\n\\nUSER: Goodbye\\nBOT: Au revoir\\n\\nUSER: I am going\\nBOT: Je vais\\n\\nUSER: great\\nBOT: génial\\n\\nUSER: I am going\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from prompt_engine.chat_engine import ChatEngine, ChatEngineConfig\n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.interaction import Interaction\n",
    "\n",
    "config = ChatEngineConfig(ModelConfig(max_tokens=1024))\n",
    "\n",
    "description = \"Convert the given english to french\"\n",
    "\n",
    "examples = [Interaction(\"Hello\", \"Bonjour\"), Interaction(\"Goodbye\", \"Au revoir\")]\n",
    "\n",
    "dialog = [Interaction(\"I am going\", \"Je vais\"), Interaction(\"great\", \"génial\")]\n",
    "\n",
    "chat_engine = ChatEngine(config = config, description = description, examples = examples, dialog = dialog)\n",
    "\n",
    "print (chat_engine.build_prompt(\"I am going\"))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Output of this example is:\n",
    "\n",
    "Convert the given english to french\n",
    "\n",
    "USER: Hello\n",
    "BOT: Bonjour\n",
    "\n",
    "USER: Goodbye\n",
    "BOT: Au revoir\n",
    "\n",
    "USER: I am going\n",
    "BOT: Je vais\n",
    "\n",
    "USER: great\n",
    "BOT: génial\n",
    "\n",
    "USER: I am going\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f340e6a-0f71-4c3d-ae79-b065f90877d2",
   "metadata": {},
   "source": [
    "\n",
    "## Example 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884f22a4-be74-4971-bc40-db14fcaf5eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\n",
      "\n",
      "## what's 10 plus 18\n",
      "print(10 + 18)\n",
      "\n",
      "## what's 10 times 18\n",
      "print(10 * 18)\n",
      "\n",
      "## what's 10 to the power of 18\n",
      "print (10 ** 18)\n",
      "\n",
      "## What's 1018 times the ninth power of four?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOutput of this example is:\\n\\n### Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\\n\\n## what's 10 plus 18\\nprint(10 + 18)\\n\\n## what's 10 times 18\\nprint(10 * 18)\\n\\n## what's 10 to the power of 18\\nprint (10 ** 18)\\n\\n## What's 1018 times the ninth power of four?\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from prompt_engine.code_engine import CodeEngine, PythonCodeEngineConfig\n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.interaction import Interaction\n",
    "\n",
    "config = PythonCodeEngineConfig(ModelConfig(max_tokens=1024))\n",
    "\n",
    "description = \"Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\"\n",
    "\n",
    "examples = [Interaction(\"what's 10 plus 18\", \"print(10 + 18)\"), Interaction(\"what's 10 times 18\", \"print(10 * 18)\")]\n",
    "\n",
    "dialog = [Interaction(\"what's 10 to the power of 18\", \"print (10 ** 18)\")]\n",
    "\n",
    "code_engine = CodeEngine(config = config, description = description, examples = examples, dialog = dialog)\n",
    "\n",
    "print (code_engine.build_prompt(\"What's 1018 times the ninth power of four?\"))\n",
    "\n",
    "\"\"\"\n",
    "Output of this example is:\n",
    "\n",
    "### Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\n",
    "\n",
    "## what's 10 plus 18\n",
    "print(10 + 18)\n",
    "\n",
    "## what's 10 times 18\n",
    "print(10 * 18)\n",
    "\n",
    "## what's 10 to the power of 18\n",
    "print (10 ** 18)\n",
    "\n",
    "## What's 1018 times the ninth power of four?\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e32ff-50b7-4927-8627-8829d767cb80",
   "metadata": {},
   "source": [
    "\n",
    "## Example 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92c2e89-48f8-4d43-a1e1-2989194b2cd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interaction \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_prompt_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicPromptEngine, PromptBank\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_Cyber_ML/lib/python3.8/site-packages/prompt_engine/dynamic_prompt_engine.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     get_embedding,\n\u001b[1;32m     15\u001b[0m     distances_from_embeddings,\n\u001b[1;32m     16\u001b[0m     indices_of_nearest_neighbors_from_distances,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "from prompt_engine.prompt_engine import PromptEngineConfig\n",
    "from prompt_engine.interaction import Interaction \n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.dynamic_prompt_engine import DynamicPromptEngine, PromptBank\n",
    "import openai\n",
    "\n",
    "api_key = \"\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "config = PromptEngineConfig(ModelConfig(max_tokens=1024), description_prefix = \"###\")\n",
    "\n",
    "description1 = \"The following prompts tell you the urgency of a notification\"\n",
    "examples1 = [Interaction(\"Your flight is going to be delayed! Please check your Delta app for updated schedules\", \"Urgent\"),\n",
    "    Interaction(\"Your daughter was just taken to the emergency room. Please call us back immediately.\", \"Urgent\"),\n",
    "    Interaction(\"Hey how are you? We should get lunch sometime.\", \"Low\"),\n",
    "    Interaction(\"What is the project status? Please send it to me today.\", \"High\"),\n",
    "    Interaction(\"Liverpool is now leading in their game vs Aston Villa.\", \"Medium\")]\n",
    "dynamic_engine = DynamicPromptEngine(openai_key = api_key, config = config, description = description1, examples=examples1, prompt_bank = PromptBank())\n",
    "del dynamic_engine\n",
    "\n",
    "description2 = \"Extract the monuments from the given text\"\n",
    "examples2 = [Interaction(\"Can we go to Taj Mahal?\", \"Taj Mahal\"),\n",
    "Interaction(\"How old is the Stonehenge?\", \"Stonehenge\"),\n",
    "Interaction(\"The Statue of Liberty is such a massive statue, I wonder how they built it\", \"Statue of Liberty\"),\n",
    "Interaction(\"What is the name of that big Buddha statue in Asia?\", \"Big Buddha Statue\"),\n",
    "Interaction(\"I want to see the Eiffel Tower!\", \"Eiffel Tower\"),\n",
    "Interaction(\"Where is The Vatican located in italy?\", \"The Vatican\"),\n",
    "Interaction(\"How many steps are there to the top of the Great Pyramid of Giza?\", \"Great Pyramid of Giza\")]\n",
    "dynamic_engine = DynamicPromptEngine(openai_key = api_key, config = config, description = description2, examples=examples2, prompt_bank = PromptBank())\n",
    "del dynamic_engine\n",
    "\n",
    "\n",
    "description = \"I want to classify the importance for the notifications\"\n",
    "flow_reset_text = \"Ignore the previous queries, start afresh\"\n",
    "dynamic_engine = DynamicPromptEngine(openai_key = api_key, config = config, description = description, flow_reset_text = flow_reset_text, prompt_bank = PromptBank())\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Enter your query: \")\n",
    "    if (user_query == \"exit\"):\n",
    "         break\n",
    "    codex_query = dynamic_engine.build_prompt(user_query)\n",
    "\n",
    "    response = openai.Completion.create(engine=\"code-davinci-002\", \n",
    "                                        prompt=codex_query, \n",
    "                                        temperature=0.3, \n",
    "                                        max_tokens=dynamic_engine.config.model_config.max_tokens, \n",
    "                                        stop=[dynamic_engine.config.newline_operator])\n",
    "\n",
    "    completion_all = response['choices'][0]['text'].strip()\n",
    "    print (codex_query+completion_all)\n",
    "    print (\"----------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    if (completion_all != \"\"):\n",
    "      dynamic_engine.add_interaction(user_query, completion_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80687b-6c87-43f3-8171-85442cbc4ea9",
   "metadata": {},
   "source": [
    "\n",
    "## Example 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67a8b9b-6492-417d-bc21-e5ada0d6f331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> I want to speak with a bot which replies in under 20 words each time\n",
      "\n",
      "Hi\n",
      "I'm a chatbot. I can chat with you about anything you'd like.\n",
      "\n",
      "Can you help me with the size of the universe?\n",
      "Sure. The universe is estimated to be around 93 billion light years in diameter.\n",
      "\n",
      "-> Forget the earlier conversation and start afresh\n",
      "\n",
      "What is the size of an SUV in general?\n",
      "An SUV typically ranges from 16 to 20 feet long.\n",
      "\n",
      "What is the maximum speed an SUV from a performance brand can achieve?\n",
      "Some performance SUVs can reach speeds over 150mph.\n",
      "\n",
      "\n",
      "I want to speak with a bot which replies in under 20 words each time\n",
      "\n",
      "-> Hi\n",
      "I'm a chatbot. I can chat with you about anything you'd like. <-\n",
      "\n",
      "-> Can you help me with the size of the universe?\n",
      "Sure. The universe is estimated to be around 93 billion light years in diameter. <-\n",
      "\n",
      "-> What is a light year?\n",
      "A light year is the distance that light can travel in one year. <-\n",
      "\n",
      "-> Can any spacecraft travel at the speed of light?\n",
      "\n",
      "I want to speak with a bot which replies in under 20 words each time\n",
      "\n",
      "-> Hi\n",
      "I'm a chatbot. I can chat with you about anything you'd like. <-\n",
      "\n",
      "-> Can you help me with the size of the universe?\n",
      "Sure. The universe is estimated to be around 93 billion light years in diameter. <-\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOutput after resetting the context but without any prompt:\\n\\nI want to speak with a bot which replies in under 20 words each time\\n\\n-> Hi\\nI'm a chatbot. I can chat with you about anything you'd like. <-\\n\\n-> Can you help me with the size of the universe?\\nSure. The universe is estimated to be around 93 billion light years in diameter. <-\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from prompt_engine.prompt_engine import PromptEngine, PromptEngineConfig\n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.interaction import Interaction\n",
    "\n",
    "config = PromptEngineConfig(ModelConfig(max_tokens=1024), description_prefix = \"->\")\n",
    "description = \"I want to speak with a bot which replies in under 20 words each time\"\n",
    "examples = [Interaction(\"Hi\", \"I'm a chatbot. I can chat with you about anything you'd like.\"), \n",
    "            Interaction(\"Can you help me with the size of the universe?\", \"Sure. The universe is estimated to be around 93 billion light years in diameter.\")]\n",
    "flow_reset_text = \"Forget the earlier conversation and start afresh\"\n",
    "dialog = [Interaction(\"What is the size of an SUV in general?\", \"An SUV typically ranges from 16 to 20 feet long.\"), \n",
    "        Interaction(\"What is the maximum speed an SUV from a performance brand can achieve?\", \"Some performance SUVs can reach speeds over 150mph.\")]\n",
    "prompt_engine = PromptEngine(config = config, description = description, examples = examples, flow_reset_text = flow_reset_text, dialog = dialog)\n",
    "\n",
    "print (prompt_engine.build_context())\n",
    "\n",
    "\"\"\"\n",
    "Output of this example is:\n",
    "\n",
    "-> I want to speak with a bot which replies in under 20 words each time\n",
    "\n",
    "Hi\n",
    "I'm a chatbot. I can chat with you about anything you'd like.\n",
    "\n",
    "Can you help me with the size of the universe?\n",
    "Sure. The universe is estimated to be around 93 billion light years in diameter.\n",
    "\n",
    "-> Forget the earlier conversation and start afresh\n",
    "\n",
    "What is the size of an SUV in general?\n",
    "An SUV typically ranges from 16 to 20 feet long.\n",
    "\n",
    "What is the maximum speed an SUV from a performance brand can achieve?\n",
    "Some performance SUVs can reach speeds over 150mph.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "config = PromptEngineConfig(ModelConfig(max_tokens=1024), input_prefix = \"->\", output_postfix=\"<-\", )\n",
    "description = \"I want to speak with a bot which replies in under 20 words each time\"\n",
    "prompt_engine = PromptEngine(config, description)\n",
    "\n",
    "prompt_engine.add_example(\"Hi\", \"I'm a chatbot. I can chat with you about anything you'd like.\")\n",
    "prompt_engine.add_example(\"Can you help me with the size of the universe?\", \"Sure. The universe is estimated to be around 93 billion light years in diameter.\")\n",
    "\n",
    "prompt_engine.add_interaction(\"What is a light year?\", \"A light year is the distance that light can travel in one year.\")\n",
    "\n",
    "print(prompt_engine.build_prompt(\"Can any spacecraft travel at the speed of light?\"))\n",
    "\n",
    "\"\"\"\n",
    "Output of this example is:\n",
    "\n",
    "I want to speak with a bot which replies in under 20 words each time\n",
    "\n",
    "-> Hi\n",
    "I'm a chatbot. I can chat with you about anything you'd like. <-\n",
    "\n",
    "-> Can you help me with the size of the universe?\n",
    "Sure. The universe is estimated to be around 93 billion light years in diameter. <-\n",
    "\n",
    "-> What is a light year?\n",
    "A light year is the distance that light can travel in one year. <-\n",
    "\n",
    "-> Can any spacecraft travel at the speed of light?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print (prompt_engine.reset_context())\n",
    "\n",
    "\"\"\"\n",
    "Output after resetting the context but without any prompt:\n",
    "\n",
    "I want to speak with a bot which replies in under 20 words each time\n",
    "\n",
    "-> Hi\n",
    "I'm a chatbot. I can chat with you about anything you'd like. <-\n",
    "\n",
    "-> Can you help me with the size of the universe?\n",
    "Sure. The universe is estimated to be around 93 billion light years in diameter. <-\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd26b7d-5456-49c0-abd4-31a5a5e5ac4a",
   "metadata": {},
   "source": [
    "\n",
    "## Example 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94723838-ab5b-4962-b91f-5fdd6ca49970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  what is a drone\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m      \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     26\u001b[0m codex_query \u001b[38;5;241m=\u001b[39m code_engine\u001b[38;5;241m.\u001b[39mbuild_prompt(user_query)\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode-davinci-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodex_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription_prefix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m completion_all \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m (codex_query \u001b[38;5;241m+\u001b[39m completion_all)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_Cyber_ML/lib/python3.8/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from statistics import mode\n",
    "from prompt_engine.code_engine import CodeEngine, PythonCodeEngineConfig\n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.interaction import Interaction\n",
    "import openai\n",
    "\n",
    "# This is an example to showcase the capabilities of the prompt-engine and how it can be easily integrated\n",
    "# into OpenAI's API for code generation\n",
    "\n",
    "# Creating OpenAI configuration\n",
    "api_key = \"\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Creating a new code engine\n",
    "config = PythonCodeEngineConfig(ModelConfig(max_tokens=1024))\n",
    "description = \"Natural Language Commands to Math Code\"\n",
    "examples = [Interaction(\"what's 10 plus 18\", \"print(10 + 18)\"), \n",
    "            Interaction(\"what's 10 times 18\", \"print(10 * 18)\")]\n",
    "code_engine = CodeEngine(config = config, description = description, examples = examples)\n",
    "\n",
    "# Creating a new readline interface\n",
    "while True:\n",
    "    user_query = input(\"Enter your query: \")\n",
    "    if (user_query == \"exit\"):\n",
    "         break\n",
    "    codex_query = code_engine.build_prompt(user_query)\n",
    "\n",
    "    response = openai.Completion.create(engine=\"code-davinci-002\", prompt=codex_query, temperature=0.3, max_tokens=code_engine.config.model_config.max_tokens, stop=[config.input_prefix, config.description_prefix])\n",
    "\n",
    "    completion_all = response['choices'][0]['text'].strip()\n",
    "    print (codex_query + completion_all)\n",
    "    print (\"----------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    if (completion_all != \"\"):\n",
    "      code_engine.add_interaction(user_query, completion_all)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365afb50-40ef-45d7-ac24-c54c501429fb",
   "metadata": {},
   "source": [
    "\n",
    "## Example 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c17b1dbc-76c0-4343-abcb-2599daaf69a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> I want to speak with a bot which replies in under 20 words each time\n",
      "\n",
      "Example: Hi\n",
      "Answer: I'm a chatbot. I can chat with you about anything you'd like.\n",
      "\n",
      "Example: Can you help me with the size of the universe?\n",
      "Answer: Sure. The universe is estimated to be around 93 billion light years in diameter.\n",
      "\n",
      "What is the size of an SUV in general?\n",
      "An SUV typically ranges from 16 to 20 feet long.\n",
      "\n",
      "What is the maximum speed an SUV from a performance brand can achieve?\n",
      "Some performance SUVs can reach speeds over 150mph.\n",
      "\n",
      "What's the most popular SUV in the world?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOutput of this example is:\\n\\n-> I want to speak with a bot which replies in under 20 words each time                                                                                                                  \\nExample: Hi\\nAnswer: I'm a chatbot. I can chat with you about anything you'd like.\\n\\nExample: Can you help me with the size of the universe?\\nAnswer: Sure. The universe is estimated to be around 93 billion light years in diameter.\\n\\nWhat is the size of an SUV in general?\\nAn SUV typically ranges from 16 to 20 feet long.\\n\\nWhat is the maximum speed an SUV from a performance brand can achieve?\\nSome performance SUVs can reach speeds over 150mph.\\n\\nWhat's the most popular SUV in the world?\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from prompt_engine.prompt_engine import PromptEngine, PromptEngineConfig\n",
    "from prompt_engine.model_config import ModelConfig\n",
    "from prompt_engine.interaction import Interaction\n",
    "\n",
    "class PromptEngineOverloaded(PromptEngine):\n",
    "    \n",
    "    def _insert_examples(self, context: str = \"\", user_input: str = \"\"):\n",
    "        \"\"\"\n",
    "        Inserts the examples into the context\n",
    "        \"\"\"\n",
    "        temp_examples_text = \"\"\n",
    "        if (self.examples != []):\n",
    "            for example in self.examples:\n",
    "\n",
    "                #Original: temp_example_text = self.config.input_prefix + example.input + self.config.input_postfix + self.config.newline_operator\n",
    "                #Replacing input_prefix and input_postfix with static values\n",
    "                temp_example_text = \"Example: \" + example.input + self.config.newline_operator\n",
    "\n",
    "                #Original: temp_example_text += self.config.output_prefix +  example.response + self.config.output_postfix +  self.config.newline_operator*2\n",
    "                #Replacing output_prefix and output_postfix with static values\n",
    "                temp_example_text += \"Answer: \" + example.response + self.config.newline_operator*2\n",
    "\n",
    "                temp_examples_text += temp_example_text\n",
    "\n",
    "            context += temp_examples_text\n",
    "        \n",
    "        return context\n",
    "\n",
    "config = PromptEngineConfig(ModelConfig(max_tokens=1024), description_prefix = \"->\")\n",
    "\n",
    "\n",
    "description = \"I want to speak with a bot which replies in under 20 words each time\"\n",
    "\n",
    "examples = [Interaction(\"Hi\", \"I'm a chatbot. I can chat with you about anything you'd like.\"), \n",
    "            Interaction(\"Can you help me with the size of the universe?\", \"Sure. The universe is estimated to be around 93 billion light years in diameter.\")]\n",
    "\n",
    "dialog = [Interaction(\"What is the size of an SUV in general?\", \"An SUV typically ranges from 16 to 20 feet long.\"), \n",
    "        Interaction(\"What is the maximum speed an SUV from a performance brand can achieve?\", \"Some performance SUVs can reach speeds over 150mph.\")]\n",
    "\n",
    "prompt_engine = PromptEngineOverloaded(config = config, description = description, examples = examples, dialog = dialog)\n",
    "\n",
    "print (prompt_engine.build_prompt(\"What's the most popular SUV in the world?\"))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Output of this example is:\n",
    "\n",
    "-> I want to speak with a bot which replies in under 20 words each time                                                                                                                  \n",
    "Example: Hi\n",
    "Answer: I'm a chatbot. I can chat with you about anything you'd like.\n",
    "\n",
    "Example: Can you help me with the size of the universe?\n",
    "Answer: Sure. The universe is estimated to be around 93 billion light years in diameter.\n",
    "\n",
    "What is the size of an SUV in general?\n",
    "An SUV typically ranges from 16 to 20 feet long.\n",
    "\n",
    "What is the maximum speed an SUV from a performance brand can achieve?\n",
    "Some performance SUVs can reach speeds over 150mph.\n",
    "\n",
    "What's the most popular SUV in the world?\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59daf7-6baf-4abe-aff0-ffee8a7df710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64874109-2b9e-4f57-86eb-1e6bfed5a2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18c33e-a26e-4b55-b019-686f23ea91e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62874c7-e77e-45c6-8c4c-fc72c4af67ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526e4c7-f0f1-4c3c-b7c0-d986c87da4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdf9ef-79e7-4e62-a41f-7b88cb73873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b70d8-e28c-4492-9869-857a4721e576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0b7a4-986b-4d73-a544-af6ee82a7391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1db038-ef8b-43e1-adc8-9e1d85f2863f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f67b5-bd3e-4b59-8a42-26823bff31a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c057ea8-5598-442f-85f2-2d8c5c144ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
